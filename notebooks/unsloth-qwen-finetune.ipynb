{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y-__DkMUd72"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5J1ArgYUd73"
   },
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ9RuflBUd73"
   },
   "source": [
    "**Read our [Gemma 3 blog](https://unsloth.ai/blog/gemma3) for what's new in Unsloth and our [Reasoning blog](https://unsloth.ai/blog/r1-reasoning) on how to train reasoning models.**\n",
    "\n",
    "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR5OD2LxUd73"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8YDaPGEYUd73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.3.18-py3-none-any.whl.metadata (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting unsloth_zoo>=2025.3.14 (from unsloth)\n",
      "  Downloading unsloth_zoo-2025.3.16-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (23.2)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.17-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting datasets>=2.16.0 (from unsloth)\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.6)\n",
      "Collecting wheel>=0.42.0 (from unsloth)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.24.1)\n",
      "Collecting accelerate>=0.34.1 (from unsloth)\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
      "  Downloading peft-0.15.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting protobuf<4.0.0 (from unsloth)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting huggingface_hub (from unsloth)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Downloading diffusers-0.32.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.16.0+cu118)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.1)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.9.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->unsloth)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.16.0->unsloth)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=2.16.0->unsloth)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2023.4.0)\n",
      "Collecting aiohttp (from datasets>=2.16.0->unsloth)\n",
      "  Downloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->unsloth) (4.4.0)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub->unsloth)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.47.0,>=4.46.1->unsloth)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,>=4.46.1->unsloth)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting rich (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.3.14->unsloth)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unsloth_zoo>=2025.3.14->unsloth) (9.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers->unsloth) (4.6.4)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.16.0->unsloth)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2022.12.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.16.0->unsloth)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.16.0->unsloth)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
      "Downloading unsloth-2025.3.18-py3-none-any.whl (192 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.15.0-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.8/410.8 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:03\u001b[0mm\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.3.16-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.9/126.9 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.29.post3-cp310-cp310-manylinux_2_28_x86_64.whl (43.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mmm\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.17-py3-none-any.whl (123 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: triton, sentencepiece, pytz, nvidia-cusparselt-cu12, xxhash, wheel, tzdata, typing-extensions, tqdm, sympy, shtab, safetensors, requests, regex, pyarrow, protobuf, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mdurl, hf_transfer, fsspec, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, typeguard, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, multidict, markdown-it-py, huggingface_hub, aiosignal, yarl, tokenizers, rich, nvidia-cusolver-cu12, diffusers, tyro, transformers, torch, aiohttp, xformers, torchvision, cut_cross_entropy, bitsandbytes, accelerate, peft, datasets, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.3\n",
      "    Uninstalling wheel-0.41.3:\n",
      "      Successfully uninstalled wheel-0.41.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.5.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 bitsandbytes-0.45.3 cut_cross_entropy-25.1.1 datasets-3.4.1 diffusers-0.32.2 dill-0.3.8 docstring-parser-0.16 frozenlist-1.5.0 fsspec-2024.12.0 hf_transfer-0.1.9 huggingface_hub-0.29.3 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.2.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 peft-0.15.0 propcache-0.3.0 protobuf-3.20.3 pyarrow-19.0.1 pytz-2025.1 regex-2024.11.6 requests-2.32.3 rich-13.9.4 safetensors-0.5.3 sentencepiece-0.2.0 shtab-1.7.1 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 torchvision-0.21.0 tqdm-4.67.1 transformers-4.50.0 triton-3.2.0 trl-0.15.2 typeguard-4.4.2 typing-extensions-4.12.2 tyro-0.9.17 tzdata-2025.2 unsloth-2025.3.18 unsloth_zoo-2025.3.16 wheel-0.45.1 xformers-0.0.29.post3 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    %pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    %pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jdDH_WYUd73"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544,
     "referenced_widgets": [
      "2ffb664f4cc94eaabdefcc2778f8060a",
      "94ab0f6e8f654121aaa1685b1e526d8d",
      "6158d01922ca4b999989369dc5901dbd",
      "e095ffbf61754944b1e432a38a77241b",
      "1dd621a66c58490596d95371da13be63",
      "14f71e50c69c4499a0c5a7c6bd3feb35",
      "6bfe62fbe0df4a8ca7ea83cd43e4bbc9",
      "4d110cafd0fc4708be2d1e5fa324f6a3",
      "28663294a7f64b698d9b5afcc890ca63",
      "3bcabd1791a5490297d785d8c14ed5c4",
      "cae6d8bfb693434eacb0fb580c997aa6",
      "ac738b8417c3457facd60fceba916651",
      "543c713342294ccfaaa8ac4cc0d7b9ea",
      "60344862b7ef4afbbbf442e1702c1174",
      "37946d747afa48b3ab0e57bc9bb803ff",
      "6f86fd33c5e648c3b1e866cb9733e859",
      "df68088159054b8fb33d12985816c174",
      "14b5768ee8434085a59f1c96291d7ee1",
      "552bcb664de946d09750eac2301843d0",
      "939ef396ba124deeb487ec688e9b405b",
      "c2e30fcec5b54710bf82fc5f2866125c",
      "ffcb3ecded52490ab0c718af0803dc4e",
      "bb07ae9ce23c4e648b303447748fa383",
      "8544c3fc19fc4f4f9528b4f98356cc6d",
      "b09e9f5413fa402986eb59a35126ca2e",
      "a1da3074d97b4a4391409ef298a0db37",
      "812b72a4a251478097a65627135cfe76",
      "a16940c99cbb4875a13704cb527a101e",
      "3d0f94f07dce4806ad4e299dbac3298b",
      "4e5fe3b9af8643b0b0918b940143ce98",
      "a0cf743397ca4489ac1c846a4ef8d0cd",
      "b820454d4c0c440e83a2b57353eff382",
      "ebd1ce73948a45ecbc0007dd25a8c8f9",
      "b9ec29c5ac0a4583a92f85336cd0af34",
      "13f031e2f4ea461b94d6886d8d8bccdf",
      "2a16ef27b3c841c395769f5a7dafc719",
      "b751e2c2987e41bbabc3b872d24e9efb",
      "0264ab38c6ad4ed6bb1bba3f86d8ed43",
      "eac8e49d8bb04acf819a4c71aa448866",
      "def6b5fffe8d4ca69366529c614df1c9",
      "dd9f3dc3b8b24d93b15bf6686afeb45a",
      "8cf4f7188a9949e18ca5fe7dd7b29427",
      "5a2690be9ae444db8939d0eeb298af9e",
      "f672d188882d4e94b1e4647e60db5a09",
      "c25f51c4387c4e239e2952731011ed40",
      "bc583ff39db240e8961bd05bb218b851",
      "92195954aba648a29746391497e3b855",
      "7ee0d4d26fea4d9cbd435b0817d7324c",
      "e8b8d64b3d8e47228b099ba3095b2696",
      "a4a2734d7a7240b990270e685b0df10a",
      "5472c7cbd74d43f795fca650da2cc19b",
      "c6530ea092a34753a8a228529fc6d2b1",
      "fac4bf65a9ab4719a64fcd9b5f7b9ef3",
      "76003bccaaa34d4abd2e516a6cd3362f",
      "7131a457d51e4b3fa5337f2279f18886",
      "338682cd8a9a455ca7e2d139388e47bd",
      "1da12e9bf2fa4218bbca2ffe47b50766",
      "7fbc14c28b3d4ca789e0429664338ccd",
      "9027f63c08d24dd3af9aa71038d8f0ba",
      "b11124231dac41f1a01da1f47991c7cb",
      "c3171cc46b344768a13bb9dedd67cc75",
      "aa850eb305834984bfe6cd3cbe5a35d9",
      "3dd92ca6ecbe4e738938db92f178fe8e",
      "a69fa82693bc4ae98d52cdf02ca8ac3d",
      "a587d1961b0c4321b36eb5eb7228ecb4",
      "8af0d2fe18784dab809d029dab964efa",
      "a573e7c1a410406d80991587eaa9f37e",
      "ce74764c2a84491694913286fbf2ab13",
      "a48595c58f754264a4cae28fa93de809",
      "64567e31203447c3a89b7e6b0bbd4c95",
      "ee12988a9618428e8ca707fa811cbd92",
      "127d33a346014869ad96f79ee8902b5c",
      "eea06e612da9482f96cc57dce0addd6d",
      "b91b4b0d5e3f445fbf23f00a97dc03a5",
      "fe57d8f6f27346199511641df64c0132",
      "0b5d8ae0345048fcbb519d9501053311",
      "13459c68e53b409aadfba2dd4ca38357",
      "4313382cd09f4ab8a83363d7295155b2",
      "e81737506cc8447a96feaf20129a1c24",
      "796683a2fad941b3b26dac8335d8da27",
      "438e9123c7964265be7b9b097b810ced",
      "ad86f1ca4213423891143beafeffbffd",
      "30f3f0f267dc44d18eebd67ab0936ea3",
      "200b1b38541648c199dea36ec90771a6",
      "c047cf8cf49c490eb283c152f5658fbe",
      "2fc5963e9bb4475eaed559b1f80ed550",
      "899269977908477583c70a137d134262",
      "c7cffeee6db14e2b87bfd6d8cf834067",
      "7c38fe2fde6847d38d393f18426cf2a9",
      "6e3b1fb46e834ad3929da636571f99f1",
      "ef129f3918cd49efb2e8fcc0f7cc0b6b",
      "159829d39d0346e09e7bc1d3e5a60199",
      "09855a1da46f4a3fa6c4d9dd791e7bc2",
      "1432c9f8407148a1991245d9e26b9415",
      "107f9ab2ba764efc8e893594a6785e49",
      "70def416cfb740b7a92b78892ffb6ea8",
      "f6b72bde546040eeaecaa01a8332fc73",
      "696e330eff5543b4bd12d8f5182b44ad",
      "030e0522ab9145e0babff4a743780e83",
      "b69ece2580104c64a966f5836b04757d",
      "749367c7cc89432f8ad7fd71721b90db",
      "0215f9900e6645fd868b26320d000af9",
      "e8fedcc3275847e0922085f78ebbf72e",
      "7878f30685674c68bee7bda5e0ba81ce",
      "852d1bc276ea42c59d27a061508182de",
      "ad120a1adb454b9994c90ff040f55175",
      "a5a690578c43453ea0c89ffdb98113f2",
      "27c667cd0f4744a49b0c1efae6a5b7f8",
      "b30a83da5cb4429e83d3d731db8511a9",
      "4d0fedfd991b4a2488fa074c962ba1e0",
      "9ba93c797737447eb1e8b5db5ffc6a0e",
      "c976f872e9e94c94b84e88d46a621234",
      "3c2451f6fbe44ecdbca80b32ff002157",
      "20e2e9c2bed047b783619a67d7233572",
      "c5cc980ca1a14054bca9c21d40810369",
      "08ae1ffe5f454223bdf9ad0ed862474b",
      "3a97b55b228f48a2869fdb0724efe6b3",
      "733791e4faf748abbe2e06110c8e2cf9",
      "1e233c94a1334072928b53a7f01b403f",
      "baede4e0c956481ca0e9db9ae57f5da0",
      "3b2f7e93c1e14b62b700650921407b0f",
      "a4572b95714f48e58a688d5dae5e8938",
      "68cbbbd02c8d407381ea91a05de72912",
      "f1285f8b776a4c4490be1f5cc9d20ad5",
      "f5ea8be95ddc436faa7ff1eaa17e20ca",
      "2ec48c5334364a7e83526743102729fa",
      "5e2ae64211e544c68df3d5c1b3e052a7",
      "c40d073d24dd437c9d32950de40366f1",
      "903381aef7994e1bb2e4198ec4bab725",
      "fec9be26c7404e4c88dbbd3c18b67dfb",
      "4ffb9a4023cd46c592607688f7b10386",
      "20573cc8eeb14a81875a5ade1e26b85b"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "f1c86a76-ca8c-4056-ecd7-f1136bc813a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-0.5B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "582000f6-4c0a-4932-816d-b1e26979550e"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "\n",
    "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb)\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d76747fb1294c76a53c225c097f364a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List future topics related to the given topic.\n",
      "\n",
      "### Input:\n",
      "Elephant\n",
      "\n",
      "### Response:\n",
      "Ecosystem,Conservation,Poaching,Circus<|endoftext|>\n",
      "------------\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List people topics related to the given topic.\n",
      "\n",
      "### Input:\n",
      "SpaceX\n",
      "\n",
      "### Response:\n",
      "Elon Musk,Engineers,Astronauts,Tesla<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Ensure tokenizer is defined earlier\n",
    "\n",
    "# Load your custom CSV dataset\n",
    "df = pd.read_csv('../data/finetune_examples.csv')\n",
    "\n",
    "# Convert DataFrame to Dataset\n",
    "custom_dataset = Dataset.from_pandas(df)\n",
    "max_output_topics = 4\n",
    "\n",
    "# Define a formatting function to match the prompt style\n",
    "def formatting_prompts_func(examples):\n",
    "    actions = examples[\"Action\"]\n",
    "    input_topics = examples[\"Input Topic\"]\n",
    "    output_topics = examples[\"Output Topics\"]\n",
    "\n",
    "    texts = []\n",
    "    for action, input_topic, output_topic in zip(actions, input_topics, output_topics):\n",
    "        instruction = f\"List {action} topics related to the given topic.\"\n",
    "        input_context = input_topic\n",
    "        response = \",\".join(output_topic.split(\",\")[:max_output_topics])\n",
    "\n",
    "        text = alpaca_prompt.format(instruction, input_context, response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return { \"text\": texts }\n",
    "\n",
    "# Apply the formatting function to your custom dataset\n",
    "dataset = custom_dataset.map(formatting_prompts_func, batched=True)\n",
    "print(dataset[\"text\"][0])\n",
    "print(\"------------\")\n",
    "print(dataset[\"text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nBroader\\n\\n### Input:\\nZoo\\n\\n### Response:\\nThe zoo is a place where animals are kept and cared for. It is a place where animals can be seen and interacted with by people. The zoo is designed to provide animals with a safe and comfortable environment to live and grow. The zoo is open 24 hours a day, seven days a week, and']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Broader\",\n",
    "        \"Zoo\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "5de8ef5863dd42c9bb5f29b9b6b7bed2",
      "a917d1ad0e954111a7bfd4181dc28386",
      "6d64ddb8e8064e8ba635a3d05727da48",
      "ce2de81db24944bf8492650e030c296b",
      "83191b1a2ea5458888b3359423b3ac4e",
      "3aa2a157bd6d43239b442756db9fdc26",
      "a5869133bfee4742a2eb50fe6aa070a6",
      "463febfcb755491e972f561c7381c60f",
      "9ea717d3cffa49fa968d5d7122a96327",
      "c566e53256e24229b7b3778dc627a0c3",
      "e519a715001444b2ba1ae48b12f00bb1",
      "19665589e86a41cebff4c6092dea9da4",
      "9d73ab40d73b424d91b2af5f1fb66288",
      "2943006158ea4d84af76dca0528137b0",
      "3e071489edf64373b6cf98adfdc24d20",
      "9b7924075e384ab7815e41b1c3cc38f4",
      "4576e8e1659d4a8993cf55d53a187bab",
      "673de1fa6e404e12a30692e99bb41b39",
      "5e95453be71546cb867211d97960f53e",
      "93975253df9e400bbdb249a3a768f40b",
      "5a00f5048ddc49e2ad998eb4c70ceec1",
      "b98a99c9e55f490a8666f480ff170f55",
      "d3f1af628dbe448e967a2707bba0865c",
      "d168052d2bb14be3a2d47ba09fb36c3e",
      "a553033b78ca4e4093004b014763e76c",
      "6a8eff3bb4164de18934ead6c5cc2f6e",
      "8090a5b79e2649eba1033a787a7e0b1e",
      "b54f2ba1f8f64019a5503ab02dfc2784",
      "b6bff295fd2e4048974e70c68febb1f6",
      "e7676f10c1ae41a6b6091cc1dd3a9d7f",
      "672c1fecf0814f639107bd13947fe5c8",
      "bee47b0e04aa4b649d370851478d5b8a",
      "6b12dff29e07450e9f064142842115e6",
      "f979c85b02b948298d1de47f9127d8fd",
      "1ba70f8a5fce450abf206d2dc4a3d1c9",
      "a4fca58037844487bc079f0daf281fd3",
      "8243271176004de4918c4fe3ee4c2554",
      "ce85e95b067b47d0a8537614addd3e9d",
      "c0ea75e0b5a24419886b7a4d3dd93cbb",
      "821e5aade7de43c494fdc216020d8aec",
      "d7e6100ad25649bf9c2a8c101826bce9",
      "5c225c767a7d4d06b2d0b135189af750",
      "c68818c1a2cb4291b279a2adee8911b3",
      "8b7c3d65d5814cd28c22aeb541d02454"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "494cc39d-96c1-48fb-b2ab-ea9b0e7033bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe89f325e4844fe593d5be0cdd86bd5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 120, # 120\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "794e68c9-8dcf-4970-903b-00feb8d08f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100 80GB PCIe. Max memory = 79.138 GB.\n",
      "2.707 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "1122c051-df9d-4196-f838-526b2a917d30"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 145 | Num Epochs = 7 | Total steps = 120\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 8,798,208/5,000,000,000 (0.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 01:08, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.621800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.053100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.830200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.826500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.786700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.743300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.496900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.544600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.532600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.525800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.953500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.447600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.374300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.349200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.327900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.357200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.311200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellView": "form",
    "id": "pCqnaKmlO1U9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.6209 seconds used for training.\n",
      "1.14 minutes used for training.\n",
      "Peak reserved memory = 2.707 GB.\n",
      "Peak reserved memory for training = 0.0 GB.\n",
      "Peak reserved memory % of max memory = 3.421 %.\n",
      "Peak reserved memory for training % of max memory = 0.0 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "kR3gIAX-SM2q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nBroader\\n\\n### Input:\\nZoo\\n\\n### Response:\\nMammals,Reptiles,Insects,Birds<|endoftext|>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Broader\",\n",
    "        \"Zoo\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "e2pEuRb1r2Vg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Deeper\n",
      "\n",
      "### Input:\n",
      "Elon Musk\n",
      "\n",
      "### Response:\n",
      "SpaceX,Deep Learning,Artificial Intelligence<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Deeper\", # instruction\n",
    "        \"Elon Musk\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ[\"HF_TOKEN\"] \n",
    "ft_model_name = \"jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "upcOlWe7A1vc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0a2a0b089c443a8b918edca75e54a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/598 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16524a7f88b74035b41a64751a47358a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067ef15157674288a781fe1080295463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c656ec0bb71443beaa8610658482415e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b58d9f596354bfe9389c58d5bb960c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.save_pretrained(\"../lora_model\")  # Local saving\n",
    "# tokenizer.save_pretrained(\"../lora_model\")\n",
    "model.push_to_hub(ft_model_name, token = HF_TOKEN) \n",
    "tokenizer.push_to_hub(ft_model_name, token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "MKX_XKs_BNZR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.50.0.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 1. Max memory: 79.138 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Broader\n",
      "\n",
      "### Input:\n",
      "Construction\n",
      "\n",
      "### Response:\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Broader\n",
      "\n",
      "### Input:\n",
      "Construction\n",
      "\n",
      "### Response:\n",
      "Building projects,Building materials,Building codes,Building materials supply chain<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = ft_model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "prompt = alpaca_prompt.format(\n",
    "        \"Broader\",\n",
    "        \"Construction\",\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "print(prompt)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        ft_model_name, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n",
    "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake\n",
      "  Downloading cmake-3.31.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Downloading cmake-3.31.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cmake\n",
      "Successfully installed cmake-3.31.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring done (8.6s)\n",
      "-- Generating done (18.8s)\n",
      "-- Build files have been written to: /workspace/finetune-llm-wikipedia-game/notebooks/llama.cpp/build\n",
      "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  3%] Built target ggml-base\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[  8%] Built target ggml-cpu\n",
      "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[  9%] Built target ggml\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 19%] Built target llama\n",
      "[ 19%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "[ 19%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[ 19%] Built target build_info\n",
      "[ 20%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 25%] Built target common\n",
      "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "[ 26%] Built target test-tokenizer-0\n",
      "[ 26%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 27%] Built target test-sampling\n",
      "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 29%] Built target test-grammar-parser\n",
      "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "[ 30%] Built target test-grammar-integration\n",
      "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 32%] Built target test-llama-grammar\n",
      "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "[ 33%] Built target test-chat\n",
      "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "[ 34%] Built target test-json-schema-to-grammar\n",
      "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 35%] Built target test-tokenizer-1-bpe\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "[ 36%] Built target test-tokenizer-1-spm\n",
      "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "[ 37%] Built target test-log\n",
      "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "[ 39%] Built target test-arg-parser\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 41%] Built target test-chat-template\n",
      "[ 41%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "[ 42%] Built target test-gguf\n",
      "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 43%] Built target test-backend-ops\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 45%] Built target test-model-load-cancel\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 46%] Built target test-autorelease\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "[ 48%] Built target test-barrier\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 49%] Built target test-quantize-fns\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 50%] Built target test-quantize-perf\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 52%] Built target test-rope\n",
      "[ 52%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
      "[ 53%] Built target test-c\n",
      "[ 54%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "[ 54%] Built target llama-batched-bench\n",
      "[ 55%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "[ 55%] Built target llama-batched\n",
      "[ 56%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "[ 56%] Built target llama-embedding\n",
      "[ 57%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "[ 57%] Built target llama-eval-callback\n",
      "[ 57%] \u001b[32mBuilding CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gbnf-validator\u001b[0m\n",
      "[ 58%] Built target llama-gbnf-validator\n",
      "[ 58%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "[ 58%] Built target sha256\n",
      "[ 59%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "[ 59%] Built target xxhash\n",
      "[ 60%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "[ 60%] Built target sha1\n",
      "[ 60%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "[ 61%] Built target llama-gguf-hash\n",
      "[ 61%] \u001b[32mBuilding CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "[ 62%] Built target llama-gguf-split\n",
      "[ 62%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "[ 63%] Built target llama-gguf\n",
      "[ 63%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
      "[ 64%] Built target llama-gritlm\n",
      "[ 64%] \u001b[32mBuilding CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "[ 64%] Built target llama-imatrix\n",
      "[ 65%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-infill\u001b[0m\n",
      "[ 65%] Built target llama-infill\n",
      "[ 66%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 66%] Built target llama-bench\n",
      "[ 67%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "[ 67%] Built target llama-lookahead\n",
      "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "[ 68%] Built target llama-lookup\n",
      "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "[ 69%] Built target llama-lookup-create\n",
      "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "[ 70%] Built target llama-lookup-merge\n",
      "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "[ 71%] Built target llama-lookup-stats\n",
      "[ 72%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "[ 72%] Built target llama-cli\n",
      "[ 72%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "[ 73%] Built target llama-parallel\n",
      "[ 73%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "[ 74%] Built target llama-passkey\n",
      "[ 74%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "[ 75%] Built target llama-perplexity\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[ 76%] Built target llama-quantize\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "[ 77%] Built target llama-retrieval\n",
      "[ 78%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
      "[ 78%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "[ 79%] Built target llama-server\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "[ 80%] Built target llama-save-load-state\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
      "[ 81%] Built target llama-run\n",
      "[ 81%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "[ 82%] Built target llama-simple\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "[ 83%] Built target llama-simple-chat\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "[ 84%] Built target llama-speculative\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "[ 85%] Built target llama-speculative-simple\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "[ 85%] Built target llama-tokenize\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "[ 86%] Built target llama-tts\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "[ 87%] Built target llama-gen-docs\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 88%] Built target llama-convert-llama2c-to-ggml\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "[ 89%] Built target llama-cvector-generator\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "[ 90%] Built target llama-export-lora\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize-stats\u001b[0m\n",
      "[ 91%] Built target llama-quantize-stats\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
      "[ 92%] Built target llava\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
      "[ 92%] Built target llava_static\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libllava_shared.so\u001b[0m\n",
      "[ 93%] Built target llava_shared\n",
      "[ 94%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "[ 94%] Built target llama-llava-cli\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "[ 95%] Built target llama-minicpmv-cli\n",
      "[ 95%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "[ 96%] Built target llama-qwen2vl-cli\n",
      "[ 96%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
      "[ 97%] Built target llama-gemma3-cli\n",
      "[ 98%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-clip-quantize-cli\u001b[0m\n",
      "[ 98%] Built target llama-llava-clip-quantize-cli\n",
      "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "[ 99%] Built target llama-vdot\n",
      "[ 99%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "[100%] Built target llama-q8dot\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/unslothai/unsloth/issues/748\n",
    "# !(cd llama.cpp; cmake -B build;cmake --build build --config Release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: Unsloth: The file ('llama.cpp/llama-quantize' or 'llama.cpp/llama-quantize.exe' if you are on Windows WSL) or 'llama.cpp/quantize' does not exist.\n",
    "# But we expect this file to exist! Maybe the llama.cpp developers changed the name or check extension of the llama-quantize file.\n",
    "# https://github.com/unslothai/unsloth/issues/1376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1134.62 out of 1511.56 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 393.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0 into bf16 GGUF format.\n",
      "The output location will be /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: llm-wikipedia-game-qwen-2.5-0.5b-v0\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 896\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 4864\n",
      "INFO:hf-to-gguf:gguf: head count = 14\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151643\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {896, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {4864, 896}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {896, 4864}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {896, 896}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {896, 128}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {896}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf: n_tensors = 290, total_size = 988.2M\n",
      "Writing: 100%|██████████| 988M/988M [00:17<00:00, 55.0Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
      "main: build = 3345 (2ee44c9a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf' to '/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.Q4_K_M.gguf' as Q4_K_M using 256 threads\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 290 tensors from /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = llm-wikipedia-game-qwen-2.5-0.5b-v0\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type bf16:  169 tensors\n",
      "[   1/ 290]                    token_embd.weight - [  896, 151936,     1,     1], type =   bf16, converting to q8_0 .. size =   259.66 MiB ->   137.94 MiB\n",
      "[   2/ 290]               blk.0.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[   3/ 290]                blk.0.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[   4/ 290]                blk.0.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[   5/ 290]                  blk.0.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[   6/ 290]                blk.0.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[   7/ 290]                    blk.0.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   8/ 290]                  blk.0.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[   9/ 290]             blk.0.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  10/ 290]                    blk.0.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  11/ 290]                  blk.0.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  12/ 290]                    blk.0.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  13/ 290]                  blk.0.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  14/ 290]               blk.1.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  15/ 290]                blk.1.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[  16/ 290]                blk.1.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  17/ 290]                  blk.1.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  18/ 290]                blk.1.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  19/ 290]                    blk.1.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  20/ 290]                  blk.1.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  21/ 290]             blk.1.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  22/ 290]                    blk.1.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  23/ 290]                  blk.1.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  24/ 290]                    blk.1.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  25/ 290]                  blk.1.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  26/ 290]              blk.10.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  27/ 290]               blk.10.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[  28/ 290]               blk.10.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  29/ 290]                 blk.10.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  30/ 290]               blk.10.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  31/ 290]                   blk.10.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  32/ 290]                 blk.10.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  33/ 290]            blk.10.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  34/ 290]                   blk.10.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  35/ 290]                 blk.10.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  36/ 290]                   blk.10.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  37/ 290]                 blk.10.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  38/ 290]              blk.11.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  39/ 290]               blk.11.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[  40/ 290]               blk.11.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  41/ 290]                 blk.11.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  42/ 290]               blk.11.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  43/ 290]                   blk.11.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  44/ 290]                 blk.11.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  45/ 290]            blk.11.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  46/ 290]                   blk.11.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  47/ 290]                 blk.11.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  48/ 290]                   blk.11.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  49/ 290]                 blk.11.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  50/ 290]              blk.12.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  51/ 290]               blk.12.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[  52/ 290]               blk.12.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  53/ 290]                 blk.12.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  54/ 290]               blk.12.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  55/ 290]                   blk.12.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  56/ 290]                 blk.12.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  57/ 290]            blk.12.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  58/ 290]                   blk.12.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  59/ 290]                 blk.12.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  60/ 290]                   blk.12.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  61/ 290]                 blk.12.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  62/ 290]              blk.13.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  63/ 290]               blk.13.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[  64/ 290]               blk.13.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  65/ 290]                 blk.13.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  66/ 290]               blk.13.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  67/ 290]                   blk.13.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  68/ 290]                 blk.13.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  69/ 290]            blk.13.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  70/ 290]                   blk.13.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  71/ 290]                 blk.13.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  72/ 290]                   blk.13.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  73/ 290]                 blk.13.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  74/ 290]              blk.14.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  75/ 290]               blk.14.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[  76/ 290]               blk.14.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  77/ 290]                 blk.14.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  78/ 290]               blk.14.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  79/ 290]                   blk.14.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  80/ 290]                 blk.14.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  81/ 290]            blk.14.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  82/ 290]                   blk.14.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  83/ 290]                 blk.14.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  84/ 290]                   blk.14.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  85/ 290]                 blk.14.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  86/ 290]              blk.15.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  87/ 290]               blk.15.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[  88/ 290]               blk.15.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  89/ 290]                 blk.15.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  90/ 290]               blk.15.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  91/ 290]                   blk.15.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  92/ 290]                 blk.15.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  93/ 290]            blk.15.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  94/ 290]                   blk.15.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  95/ 290]                 blk.15.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[  96/ 290]                   blk.15.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  97/ 290]                 blk.15.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  98/ 290]              blk.16.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  99/ 290]               blk.16.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 100/ 290]               blk.16.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 101/ 290]                 blk.16.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 102/ 290]               blk.16.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 103/ 290]                   blk.16.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 104/ 290]                 blk.16.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 105/ 290]            blk.16.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 106/ 290]                   blk.16.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 107/ 290]                 blk.16.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 108/ 290]                   blk.16.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 109/ 290]                 blk.16.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 110/ 290]              blk.17.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 111/ 290]               blk.17.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 112/ 290]               blk.17.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 113/ 290]                 blk.17.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 114/ 290]               blk.17.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 115/ 290]                   blk.17.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 116/ 290]                 blk.17.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 117/ 290]            blk.17.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 118/ 290]                   blk.17.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 119/ 290]                 blk.17.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 120/ 290]                   blk.17.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 121/ 290]                 blk.17.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 122/ 290]              blk.18.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 123/ 290]               blk.18.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 124/ 290]               blk.18.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 125/ 290]                 blk.18.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 126/ 290]               blk.18.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 127/ 290]                   blk.18.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 128/ 290]                 blk.18.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 129/ 290]            blk.18.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 130/ 290]                   blk.18.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 131/ 290]                 blk.18.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 132/ 290]                   blk.18.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 133/ 290]                 blk.18.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 134/ 290]              blk.19.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 135/ 290]               blk.19.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 136/ 290]               blk.19.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 137/ 290]                 blk.19.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 138/ 290]               blk.19.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 139/ 290]                   blk.19.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 140/ 290]                 blk.19.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 141/ 290]            blk.19.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 142/ 290]                   blk.19.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 143/ 290]                 blk.19.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 144/ 290]                   blk.19.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 145/ 290]                 blk.19.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 146/ 290]               blk.2.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 147/ 290]                blk.2.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 148/ 290]                blk.2.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 149/ 290]                  blk.2.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 150/ 290]                blk.2.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 151/ 290]                    blk.2.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 152/ 290]                  blk.2.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 153/ 290]             blk.2.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 154/ 290]                    blk.2.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 155/ 290]                  blk.2.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 156/ 290]                    blk.2.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 157/ 290]                  blk.2.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 158/ 290]              blk.20.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 159/ 290]               blk.20.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 160/ 290]               blk.20.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 161/ 290]                 blk.20.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 162/ 290]               blk.20.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 163/ 290]                   blk.20.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 164/ 290]                 blk.20.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 165/ 290]            blk.20.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 166/ 290]                   blk.20.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 167/ 290]                 blk.20.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 168/ 290]                   blk.20.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 169/ 290]                 blk.20.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 170/ 290]              blk.21.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 171/ 290]               blk.21.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 172/ 290]               blk.21.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 173/ 290]                 blk.21.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 174/ 290]               blk.21.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 175/ 290]                   blk.21.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 176/ 290]                 blk.21.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 177/ 290]            blk.21.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 178/ 290]                   blk.21.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 179/ 290]                 blk.21.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 180/ 290]                   blk.21.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 181/ 290]                 blk.21.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 182/ 290]              blk.22.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 183/ 290]               blk.22.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 184/ 290]               blk.22.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 185/ 290]                 blk.22.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 186/ 290]               blk.22.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 187/ 290]                   blk.22.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 188/ 290]                 blk.22.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 189/ 290]            blk.22.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 190/ 290]                   blk.22.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 191/ 290]                 blk.22.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 192/ 290]                   blk.22.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 193/ 290]                 blk.22.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 194/ 290]              blk.23.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 195/ 290]               blk.23.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 196/ 290]               blk.23.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 197/ 290]                 blk.23.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 198/ 290]               blk.23.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 199/ 290]                   blk.23.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 200/ 290]                 blk.23.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 201/ 290]            blk.23.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 202/ 290]                   blk.23.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 203/ 290]                 blk.23.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 204/ 290]                   blk.23.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 205/ 290]                 blk.23.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 206/ 290]               blk.3.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 207/ 290]                blk.3.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 208/ 290]                blk.3.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 209/ 290]                  blk.3.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 210/ 290]                blk.3.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 211/ 290]                    blk.3.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 212/ 290]                  blk.3.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 213/ 290]             blk.3.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 214/ 290]                    blk.3.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 215/ 290]                  blk.3.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 216/ 290]                    blk.3.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 217/ 290]                  blk.3.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 218/ 290]               blk.4.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 219/ 290]                blk.4.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 220/ 290]                blk.4.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 221/ 290]                  blk.4.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 222/ 290]                blk.4.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 223/ 290]                    blk.4.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 224/ 290]                  blk.4.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 225/ 290]             blk.4.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 226/ 290]                    blk.4.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 227/ 290]                  blk.4.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 228/ 290]                    blk.4.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 229/ 290]                  blk.4.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 230/ 290]               blk.5.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 231/ 290]                blk.5.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q4_K .. size =     8.31 MiB ->     2.34 MiB\n",
      "[ 232/ 290]                blk.5.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 233/ 290]                  blk.5.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 234/ 290]                blk.5.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 235/ 290]                    blk.5.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 236/ 290]                  blk.5.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 237/ 290]             blk.5.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 238/ 290]                    blk.5.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 239/ 290]                  blk.5.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 240/ 290]                    blk.5.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 241/ 290]                  blk.5.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 242/ 290]               blk.6.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 243/ 290]                blk.6.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 244/ 290]                blk.6.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 245/ 290]                  blk.6.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 246/ 290]                blk.6.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 247/ 290]                    blk.6.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 248/ 290]                  blk.6.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 249/ 290]             blk.6.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 250/ 290]                    blk.6.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 251/ 290]                  blk.6.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 252/ 290]                    blk.6.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 253/ 290]                  blk.6.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 254/ 290]               blk.7.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 255/ 290]                blk.7.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 256/ 290]                blk.7.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 257/ 290]                  blk.7.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 258/ 290]                blk.7.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 259/ 290]                    blk.7.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 260/ 290]                  blk.7.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 261/ 290]             blk.7.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 262/ 290]                    blk.7.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 263/ 290]                  blk.7.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 264/ 290]                    blk.7.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 265/ 290]                  blk.7.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 266/ 290]               blk.8.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 267/ 290]                blk.8.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 268/ 290]                blk.8.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 269/ 290]                  blk.8.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 270/ 290]                blk.8.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 271/ 290]                    blk.8.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 272/ 290]                  blk.8.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 273/ 290]             blk.8.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 274/ 290]                    blk.8.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 275/ 290]                  blk.8.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 276/ 290]                    blk.8.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 277/ 290]                  blk.8.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 278/ 290]               blk.9.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 279/ 290]                blk.9.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 280/ 290]                blk.9.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 281/ 290]                  blk.9.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 282/ 290]                blk.9.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 283/ 290]                    blk.9.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 284/ 290]                  blk.9.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 285/ 290]             blk.9.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 286/ 290]                    blk.9.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 287/ 290]                  blk.9.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q4_K - using fallback quantization q5_0\n",
      "converting to q5_0 .. size =     1.53 MiB ->     0.53 MiB\n",
      "[ 288/ 290]                    blk.9.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 289/ 290]                  blk.9.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 290/ 290]                   output_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "llama_model_quantize_internal: model size  =   942.43 MB\n",
      "llama_model_quantize_internal: quant size  =   373.71 MB\n",
      "llama_model_quantize_internal: WARNING: 144 of 168 tensor(s) required fallback quantization\n",
      "\n",
      "main: quantize time =  5155.09 ms\n",
      "main:    total time =  5155.09 ms\n",
      "Unsloth: Conversion completed! Output location: /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.Q4_K_M.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q8_0. This might take 20 minutes...\n",
      "main: build = 3345 (2ee44c9a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf' to '/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.Q8_0.gguf' as Q8_0 using 256 threads\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 290 tensors from /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = llm-wikipedia-game-qwen-2.5-0.5b-v0\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type bf16:  169 tensors\n",
      "[   1/ 290]                    token_embd.weight - [  896, 151936,     1,     1], type =   bf16, converting to q8_0 .. size =   259.66 MiB ->   137.94 MiB\n",
      "[   2/ 290]               blk.0.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[   3/ 290]                blk.0.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[   4/ 290]                blk.0.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[   5/ 290]                  blk.0.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[   6/ 290]                blk.0.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[   7/ 290]                    blk.0.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   8/ 290]                  blk.0.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[   9/ 290]             blk.0.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  10/ 290]                    blk.0.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  11/ 290]                  blk.0.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  12/ 290]                    blk.0.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  13/ 290]                  blk.0.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  14/ 290]               blk.1.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  15/ 290]                blk.1.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  16/ 290]                blk.1.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  17/ 290]                  blk.1.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  18/ 290]                blk.1.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  19/ 290]                    blk.1.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  20/ 290]                  blk.1.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  21/ 290]             blk.1.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  22/ 290]                    blk.1.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  23/ 290]                  blk.1.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  24/ 290]                    blk.1.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  25/ 290]                  blk.1.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  26/ 290]              blk.10.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  27/ 290]               blk.10.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  28/ 290]               blk.10.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  29/ 290]                 blk.10.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  30/ 290]               blk.10.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  31/ 290]                   blk.10.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  32/ 290]                 blk.10.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  33/ 290]            blk.10.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  34/ 290]                   blk.10.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  35/ 290]                 blk.10.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  36/ 290]                   blk.10.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  37/ 290]                 blk.10.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  38/ 290]              blk.11.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  39/ 290]               blk.11.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  40/ 290]               blk.11.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  41/ 290]                 blk.11.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  42/ 290]               blk.11.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  43/ 290]                   blk.11.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  44/ 290]                 blk.11.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  45/ 290]            blk.11.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  46/ 290]                   blk.11.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  47/ 290]                 blk.11.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  48/ 290]                   blk.11.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  49/ 290]                 blk.11.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  50/ 290]              blk.12.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  51/ 290]               blk.12.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  52/ 290]               blk.12.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  53/ 290]                 blk.12.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  54/ 290]               blk.12.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  55/ 290]                   blk.12.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  56/ 290]                 blk.12.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  57/ 290]            blk.12.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  58/ 290]                   blk.12.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  59/ 290]                 blk.12.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  60/ 290]                   blk.12.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  61/ 290]                 blk.12.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  62/ 290]              blk.13.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  63/ 290]               blk.13.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  64/ 290]               blk.13.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  65/ 290]                 blk.13.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  66/ 290]               blk.13.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  67/ 290]                   blk.13.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  68/ 290]                 blk.13.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  69/ 290]            blk.13.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  70/ 290]                   blk.13.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  71/ 290]                 blk.13.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  72/ 290]                   blk.13.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  73/ 290]                 blk.13.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  74/ 290]              blk.14.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  75/ 290]               blk.14.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  76/ 290]               blk.14.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  77/ 290]                 blk.14.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  78/ 290]               blk.14.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  79/ 290]                   blk.14.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  80/ 290]                 blk.14.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  81/ 290]            blk.14.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  82/ 290]                   blk.14.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  83/ 290]                 blk.14.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  84/ 290]                   blk.14.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  85/ 290]                 blk.14.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  86/ 290]              blk.15.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  87/ 290]               blk.15.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  88/ 290]               blk.15.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  89/ 290]                 blk.15.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[  90/ 290]               blk.15.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  91/ 290]                   blk.15.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  92/ 290]                 blk.15.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  93/ 290]            blk.15.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  94/ 290]                   blk.15.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  95/ 290]                 blk.15.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[  96/ 290]                   blk.15.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  97/ 290]                 blk.15.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  98/ 290]              blk.16.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  99/ 290]               blk.16.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 100/ 290]               blk.16.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 101/ 290]                 blk.16.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 102/ 290]               blk.16.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 103/ 290]                   blk.16.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 104/ 290]                 blk.16.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 105/ 290]            blk.16.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 106/ 290]                   blk.16.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 107/ 290]                 blk.16.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 108/ 290]                   blk.16.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 109/ 290]                 blk.16.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 110/ 290]              blk.17.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 111/ 290]               blk.17.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 112/ 290]               blk.17.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 113/ 290]                 blk.17.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 114/ 290]               blk.17.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 115/ 290]                   blk.17.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 116/ 290]                 blk.17.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 117/ 290]            blk.17.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 118/ 290]                   blk.17.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 119/ 290]                 blk.17.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 120/ 290]                   blk.17.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 121/ 290]                 blk.17.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 122/ 290]              blk.18.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 123/ 290]               blk.18.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 124/ 290]               blk.18.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 125/ 290]                 blk.18.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 126/ 290]               blk.18.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 127/ 290]                   blk.18.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 128/ 290]                 blk.18.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 129/ 290]            blk.18.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 130/ 290]                   blk.18.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 131/ 290]                 blk.18.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 132/ 290]                   blk.18.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 133/ 290]                 blk.18.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 134/ 290]              blk.19.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 135/ 290]               blk.19.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 136/ 290]               blk.19.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 137/ 290]                 blk.19.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 138/ 290]               blk.19.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 139/ 290]                   blk.19.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 140/ 290]                 blk.19.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 141/ 290]            blk.19.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 142/ 290]                   blk.19.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 143/ 290]                 blk.19.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 144/ 290]                   blk.19.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 145/ 290]                 blk.19.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 146/ 290]               blk.2.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 147/ 290]                blk.2.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 148/ 290]                blk.2.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 149/ 290]                  blk.2.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 150/ 290]                blk.2.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 151/ 290]                    blk.2.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 152/ 290]                  blk.2.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 153/ 290]             blk.2.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 154/ 290]                    blk.2.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 155/ 290]                  blk.2.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 156/ 290]                    blk.2.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 157/ 290]                  blk.2.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 158/ 290]              blk.20.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 159/ 290]               blk.20.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 160/ 290]               blk.20.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 161/ 290]                 blk.20.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 162/ 290]               blk.20.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 163/ 290]                   blk.20.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 164/ 290]                 blk.20.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 165/ 290]            blk.20.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 166/ 290]                   blk.20.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 167/ 290]                 blk.20.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 168/ 290]                   blk.20.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 169/ 290]                 blk.20.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 170/ 290]              blk.21.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 171/ 290]               blk.21.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 172/ 290]               blk.21.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 173/ 290]                 blk.21.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 174/ 290]               blk.21.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 175/ 290]                   blk.21.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 176/ 290]                 blk.21.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 177/ 290]            blk.21.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 178/ 290]                   blk.21.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 179/ 290]                 blk.21.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 180/ 290]                   blk.21.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 181/ 290]                 blk.21.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 182/ 290]              blk.22.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 183/ 290]               blk.22.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 184/ 290]               blk.22.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 185/ 290]                 blk.22.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 186/ 290]               blk.22.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 187/ 290]                   blk.22.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 188/ 290]                 blk.22.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 189/ 290]            blk.22.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 190/ 290]                   blk.22.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 191/ 290]                 blk.22.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 192/ 290]                   blk.22.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 193/ 290]                 blk.22.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 194/ 290]              blk.23.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 195/ 290]               blk.23.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 196/ 290]               blk.23.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 197/ 290]                 blk.23.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 198/ 290]               blk.23.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 199/ 290]                   blk.23.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 200/ 290]                 blk.23.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 201/ 290]            blk.23.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 202/ 290]                   blk.23.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 203/ 290]                 blk.23.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 204/ 290]                   blk.23.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 205/ 290]                 blk.23.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 206/ 290]               blk.3.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 207/ 290]                blk.3.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 208/ 290]                blk.3.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 209/ 290]                  blk.3.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 210/ 290]                blk.3.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 211/ 290]                    blk.3.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 212/ 290]                  blk.3.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 213/ 290]             blk.3.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 214/ 290]                    blk.3.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 215/ 290]                  blk.3.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 216/ 290]                    blk.3.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 217/ 290]                  blk.3.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 218/ 290]               blk.4.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 219/ 290]                blk.4.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 220/ 290]                blk.4.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 221/ 290]                  blk.4.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 222/ 290]                blk.4.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 223/ 290]                    blk.4.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 224/ 290]                  blk.4.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 225/ 290]             blk.4.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 226/ 290]                    blk.4.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 227/ 290]                  blk.4.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 228/ 290]                    blk.4.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 229/ 290]                  blk.4.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 230/ 290]               blk.5.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 231/ 290]                blk.5.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 232/ 290]                blk.5.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 233/ 290]                  blk.5.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 234/ 290]                blk.5.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 235/ 290]                    blk.5.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 236/ 290]                  blk.5.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 237/ 290]             blk.5.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 238/ 290]                    blk.5.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 239/ 290]                  blk.5.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 240/ 290]                    blk.5.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 241/ 290]                  blk.5.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 242/ 290]               blk.6.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 243/ 290]                blk.6.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 244/ 290]                blk.6.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 245/ 290]                  blk.6.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 246/ 290]                blk.6.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 247/ 290]                    blk.6.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 248/ 290]                  blk.6.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 249/ 290]             blk.6.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 250/ 290]                    blk.6.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 251/ 290]                  blk.6.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 252/ 290]                    blk.6.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 253/ 290]                  blk.6.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 254/ 290]               blk.7.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 255/ 290]                blk.7.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 256/ 290]                blk.7.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 257/ 290]                  blk.7.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 258/ 290]                blk.7.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 259/ 290]                    blk.7.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 260/ 290]                  blk.7.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 261/ 290]             blk.7.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 262/ 290]                    blk.7.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 263/ 290]                  blk.7.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 264/ 290]                    blk.7.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 265/ 290]                  blk.7.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 266/ 290]               blk.8.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 267/ 290]                blk.8.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 268/ 290]                blk.8.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 269/ 290]                  blk.8.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 270/ 290]                blk.8.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 271/ 290]                    blk.8.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 272/ 290]                  blk.8.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 273/ 290]             blk.8.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 274/ 290]                    blk.8.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 275/ 290]                  blk.8.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 276/ 290]                    blk.8.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 277/ 290]                  blk.8.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 278/ 290]               blk.9.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 279/ 290]                blk.9.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 280/ 290]                blk.9.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 281/ 290]                  blk.9.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, converting to q8_0 .. size =     8.31 MiB ->     4.42 MiB\n",
      "[ 282/ 290]                blk.9.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 283/ 290]                    blk.9.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 284/ 290]                  blk.9.attn_k.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 285/ 290]             blk.9.attn_output.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 286/ 290]                    blk.9.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 287/ 290]                  blk.9.attn_q.weight - [  896,   896,     1,     1], type =   bf16, converting to q8_0 .. size =     1.53 MiB ->     0.81 MiB\n",
      "[ 288/ 290]                    blk.9.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 289/ 290]                  blk.9.attn_v.weight - [  896,   128,     1,     1], type =   bf16, converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 290/ 290]                   output_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "llama_model_quantize_internal: model size  =   942.43 MB\n",
      "llama_model_quantize_internal: quant size  =   500.79 MB\n",
      "\n",
      "main: quantize time =  4362.86 ms\n",
      "main:    total time =  4362.86 ms\n",
      "Unsloth: Conversion completed! Output location: /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.Q8_0.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
      "main: build = 3345 (2ee44c9a)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf' to '/workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.Q5_K_M.gguf' as Q5_K_M using 256 threads\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 290 tensors from /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = llm-wikipedia-game-qwen-2.5-0.5b-v0\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type bf16:  169 tensors\n",
      "[   1/ 290]                    token_embd.weight - [  896, 151936,     1,     1], type =   bf16, converting to q8_0 .. size =   259.66 MiB ->   137.94 MiB\n",
      "[   2/ 290]               blk.0.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[   3/ 290]                blk.0.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[   4/ 290]                blk.0.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[   5/ 290]                  blk.0.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[   6/ 290]                blk.0.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[   7/ 290]                    blk.0.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   8/ 290]                  blk.0.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[   9/ 290]             blk.0.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  10/ 290]                    blk.0.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  11/ 290]                  blk.0.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  12/ 290]                    blk.0.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  13/ 290]                  blk.0.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  14/ 290]               blk.1.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  15/ 290]                blk.1.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[  16/ 290]                blk.1.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  17/ 290]                  blk.1.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  18/ 290]                blk.1.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  19/ 290]                    blk.1.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  20/ 290]                  blk.1.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  21/ 290]             blk.1.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  22/ 290]                    blk.1.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  23/ 290]                  blk.1.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  24/ 290]                    blk.1.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  25/ 290]                  blk.1.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  26/ 290]              blk.10.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  27/ 290]               blk.10.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[  28/ 290]               blk.10.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  29/ 290]                 blk.10.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  30/ 290]               blk.10.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  31/ 290]                   blk.10.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  32/ 290]                 blk.10.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  33/ 290]            blk.10.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  34/ 290]                   blk.10.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  35/ 290]                 blk.10.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  36/ 290]                   blk.10.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  37/ 290]                 blk.10.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  38/ 290]              blk.11.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  39/ 290]               blk.11.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  40/ 290]               blk.11.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  41/ 290]                 blk.11.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  42/ 290]               blk.11.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  43/ 290]                   blk.11.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  44/ 290]                 blk.11.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  45/ 290]            blk.11.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  46/ 290]                   blk.11.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  47/ 290]                 blk.11.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  48/ 290]                   blk.11.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  49/ 290]                 blk.11.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  50/ 290]              blk.12.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  51/ 290]               blk.12.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  52/ 290]               blk.12.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  53/ 290]                 blk.12.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  54/ 290]               blk.12.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  55/ 290]                   blk.12.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  56/ 290]                 blk.12.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  57/ 290]            blk.12.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  58/ 290]                   blk.12.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  59/ 290]                 blk.12.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  60/ 290]                   blk.12.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  61/ 290]                 blk.12.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  62/ 290]              blk.13.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  63/ 290]               blk.13.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[  64/ 290]               blk.13.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  65/ 290]                 blk.13.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  66/ 290]               blk.13.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  67/ 290]                   blk.13.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  68/ 290]                 blk.13.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  69/ 290]            blk.13.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  70/ 290]                   blk.13.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  71/ 290]                 blk.13.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  72/ 290]                   blk.13.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  73/ 290]                 blk.13.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[  74/ 290]              blk.14.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  75/ 290]               blk.14.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  76/ 290]               blk.14.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  77/ 290]                 blk.14.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  78/ 290]               blk.14.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  79/ 290]                   blk.14.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  80/ 290]                 blk.14.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  81/ 290]            blk.14.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  82/ 290]                   blk.14.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  83/ 290]                 blk.14.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  84/ 290]                   blk.14.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  85/ 290]                 blk.14.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  86/ 290]              blk.15.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  87/ 290]               blk.15.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[  88/ 290]               blk.15.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  89/ 290]                 blk.15.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[  90/ 290]               blk.15.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  91/ 290]                   blk.15.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  92/ 290]                 blk.15.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  93/ 290]            blk.15.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  94/ 290]                   blk.15.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  95/ 290]                 blk.15.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[  96/ 290]                   blk.15.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[  97/ 290]                 blk.15.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[  98/ 290]              blk.16.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[  99/ 290]               blk.16.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 100/ 290]               blk.16.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 101/ 290]                 blk.16.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 102/ 290]               blk.16.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 103/ 290]                   blk.16.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 104/ 290]                 blk.16.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 105/ 290]            blk.16.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 106/ 290]                   blk.16.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 107/ 290]                 blk.16.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 108/ 290]                   blk.16.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 109/ 290]                 blk.16.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 110/ 290]              blk.17.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 111/ 290]               blk.17.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 112/ 290]               blk.17.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 113/ 290]                 blk.17.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 114/ 290]               blk.17.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 115/ 290]                   blk.17.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 116/ 290]                 blk.17.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 117/ 290]            blk.17.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 118/ 290]                   blk.17.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 119/ 290]                 blk.17.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 120/ 290]                   blk.17.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 121/ 290]                 blk.17.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 122/ 290]              blk.18.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 123/ 290]               blk.18.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 124/ 290]               blk.18.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 125/ 290]                 blk.18.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 126/ 290]               blk.18.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 127/ 290]                   blk.18.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 128/ 290]                 blk.18.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 129/ 290]            blk.18.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 130/ 290]                   blk.18.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 131/ 290]                 blk.18.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 132/ 290]                   blk.18.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 133/ 290]                 blk.18.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 134/ 290]              blk.19.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 135/ 290]               blk.19.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 136/ 290]               blk.19.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 137/ 290]                 blk.19.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 138/ 290]               blk.19.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 139/ 290]                   blk.19.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 140/ 290]                 blk.19.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 141/ 290]            blk.19.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 142/ 290]                   blk.19.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 143/ 290]                 blk.19.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 144/ 290]                   blk.19.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 145/ 290]                 blk.19.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 146/ 290]               blk.2.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 147/ 290]                blk.2.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 148/ 290]                blk.2.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 149/ 290]                  blk.2.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 150/ 290]                blk.2.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 151/ 290]                    blk.2.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 152/ 290]                  blk.2.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 153/ 290]             blk.2.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 154/ 290]                    blk.2.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 155/ 290]                  blk.2.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 156/ 290]                    blk.2.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 157/ 290]                  blk.2.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 158/ 290]              blk.20.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 159/ 290]               blk.20.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 160/ 290]               blk.20.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 161/ 290]                 blk.20.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 162/ 290]               blk.20.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 163/ 290]                   blk.20.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 164/ 290]                 blk.20.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 165/ 290]            blk.20.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 166/ 290]                   blk.20.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 167/ 290]                 blk.20.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 168/ 290]                   blk.20.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 169/ 290]                 blk.20.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 170/ 290]              blk.21.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 171/ 290]               blk.21.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 172/ 290]               blk.21.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 173/ 290]                 blk.21.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 174/ 290]               blk.21.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 175/ 290]                   blk.21.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 176/ 290]                 blk.21.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 177/ 290]            blk.21.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 178/ 290]                   blk.21.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 179/ 290]                 blk.21.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 180/ 290]                   blk.21.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 181/ 290]                 blk.21.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 182/ 290]              blk.22.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 183/ 290]               blk.22.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 184/ 290]               blk.22.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 185/ 290]                 blk.22.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 186/ 290]               blk.22.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 187/ 290]                   blk.22.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 188/ 290]                 blk.22.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 189/ 290]            blk.22.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 190/ 290]                   blk.22.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 191/ 290]                 blk.22.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 192/ 290]                   blk.22.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 193/ 290]                 blk.22.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 194/ 290]              blk.23.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 195/ 290]               blk.23.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 196/ 290]               blk.23.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 197/ 290]                 blk.23.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 198/ 290]               blk.23.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 199/ 290]                   blk.23.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 200/ 290]                 blk.23.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 201/ 290]            blk.23.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 202/ 290]                   blk.23.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 203/ 290]                 blk.23.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 204/ 290]                   blk.23.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 205/ 290]                 blk.23.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 206/ 290]               blk.3.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 207/ 290]                blk.3.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 208/ 290]                blk.3.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 209/ 290]                  blk.3.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 210/ 290]                blk.3.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 211/ 290]                    blk.3.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 212/ 290]                  blk.3.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 213/ 290]             blk.3.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 214/ 290]                    blk.3.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 215/ 290]                  blk.3.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 216/ 290]                    blk.3.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 217/ 290]                  blk.3.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 218/ 290]               blk.4.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 219/ 290]                blk.4.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 220/ 290]                blk.4.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 221/ 290]                  blk.4.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 222/ 290]                blk.4.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 223/ 290]                    blk.4.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 224/ 290]                  blk.4.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 225/ 290]             blk.4.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 226/ 290]                    blk.4.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 227/ 290]                  blk.4.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 228/ 290]                    blk.4.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 229/ 290]                  blk.4.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 230/ 290]               blk.5.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 231/ 290]                blk.5.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q5_K .. size =     8.31 MiB ->     2.86 MiB\n",
      "[ 232/ 290]                blk.5.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 233/ 290]                  blk.5.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 234/ 290]                blk.5.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 235/ 290]                    blk.5.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 236/ 290]                  blk.5.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 237/ 290]             blk.5.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 238/ 290]                    blk.5.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 239/ 290]                  blk.5.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 240/ 290]                    blk.5.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 241/ 290]                  blk.5.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 242/ 290]               blk.6.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 243/ 290]                blk.6.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 244/ 290]                blk.6.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 245/ 290]                  blk.6.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 246/ 290]                blk.6.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 247/ 290]                    blk.6.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 248/ 290]                  blk.6.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 249/ 290]             blk.6.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 250/ 290]                    blk.6.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 251/ 290]                  blk.6.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 252/ 290]                    blk.6.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 253/ 290]                  blk.6.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 254/ 290]               blk.7.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 255/ 290]                blk.7.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 256/ 290]                blk.7.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 257/ 290]                  blk.7.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 258/ 290]                blk.7.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 259/ 290]                    blk.7.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 260/ 290]                  blk.7.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 261/ 290]             blk.7.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 262/ 290]                    blk.7.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 263/ 290]                  blk.7.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 264/ 290]                    blk.7.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 265/ 290]                  blk.7.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 266/ 290]               blk.8.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 267/ 290]                blk.8.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 268/ 290]                blk.8.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 269/ 290]                  blk.8.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 270/ 290]                blk.8.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 271/ 290]                    blk.8.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 272/ 290]                  blk.8.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 273/ 290]             blk.8.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 274/ 290]                    blk.8.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 275/ 290]                  blk.8.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 276/ 290]                    blk.8.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 277/ 290]                  blk.8.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 278/ 290]               blk.9.attn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 279/ 290]                blk.9.ffn_down.weight - [ 4864,   896,     1,     1], type =   bf16, converting to q6_K .. size =     8.31 MiB ->     3.41 MiB\n",
      "[ 280/ 290]                blk.9.ffn_gate.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 281/ 290]                  blk.9.ffn_up.weight - [  896,  4864,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 4864 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     8.31 MiB ->     3.12 MiB\n",
      "[ 282/ 290]                blk.9.ffn_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 283/ 290]                    blk.9.attn_k.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 284/ 290]                  blk.9.attn_k.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     0.22 MiB ->     0.08 MiB\n",
      "[ 285/ 290]             blk.9.attn_output.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 286/ 290]                    blk.9.attn_q.bias - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "[ 287/ 290]                  blk.9.attn_q.weight - [  896,   896,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 896 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =     1.53 MiB ->     0.57 MiB\n",
      "[ 288/ 290]                    blk.9.attn_v.bias - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[ 289/ 290]                  blk.9.attn_v.weight - [  896,   128,     1,     1], type =   bf16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 896 x 128 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =     0.22 MiB ->     0.12 MiB\n",
      "[ 290/ 290]                   output_norm.weight - [  896,     1,     1,     1], type =    f32, size =    0.003 MB\n",
      "llama_model_quantize_internal: model size  =   942.43 MB\n",
      "llama_model_quantize_internal: quant size  =   394.95 MB\n",
      "llama_model_quantize_internal: WARNING: 144 of 168 tensor(s) required fallback quantization\n",
      "\n",
      "main: quantize time =  4253.01 ms\n",
      "main:    total time =  4253.01 ms\n",
      "Unsloth: Conversion completed! Output location: /workspace/finetune-llm-wikipedia-game/notebooks/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0/unsloth.Q5_K_M.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e759e1e1e44cdcb95ed499c7a4dd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9ff9abbbd44e4c99839aabdf9f1abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q4_K_M.gguf:   0%|          | 0.00/398M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eff412a7754921b4ca1c856c6c6a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9c7207b4674cdabfe0052269e98103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q8_0.gguf:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d52b7e428a34b8bac63dcf14db0441b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21a9c1282a54fcb9897083183a32564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q5_K_M.gguf:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/jmcallister/llm-wikipedia-game-qwen-2.5-0.5b-v0\n"
     ]
    }
   ],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(ft_model_name, tokenizer,)\n",
    "if False: model.push_to_hub_gguf(ft_model_name, tokenizer, token = HF_TOKEN)\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(ft_model_name, tokenizer, quantization_method = \"f16\", token = HF_TOKEN)\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(ft_model_name, tokenizer, quantization_method = \"q4_k_m\", token = HF_TOKEN)\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if True:\n",
    "    model.push_to_hub_gguf(\n",
    "        ft_model_name, # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = HF_TOKEN, # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENgUDKztUd75"
   },
   "source": [
    "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
    "\n",
    "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
    "\n",
    "Some other links:\n",
    "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
    "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
    "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
    "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
    "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
    "\n",
    "  Join Discord if you need help + ⭐️ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐️\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
